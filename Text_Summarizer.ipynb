{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Text Summarizer:**"],"metadata":{"id":"GLuIxLp62Ngv"}},{"cell_type":"markdown","source":["Description: NLP text summarization is the process of breaking down lengthy text into digestible paragraphs or sentences. This method extracts vital information while also preserving the meaning of the text. This reduces the time required for grasping lengthy pieces such as articles without losing vital information."],"metadata":{"id":"zGy2B6do2SVR"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoPLDXd4OBMj","outputId":"e0f56958-0e91-4a9b-ba78-401b6626554f","executionInfo":{"status":"ok","timestamp":1681929989308,"user_tz":-330,"elapsed":30879,"user":{"displayName":"Sreya Reddy","userId":"13301942800402694680"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sumy\n","  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycountry>=18.2.23\n","  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from sumy) (3.8.1)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.9/dist-packages (from sumy) (2.27.1)\n","Collecting breadability>=0.1.20\n","  Downloading breadability-0.1.20.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting docopt<0.7,>=0.6.1\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: chardet in /usr/local/lib/python3.9/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.9/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.0.2->sumy) (8.1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pycountry>=18.2.23->sumy) (67.6.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.7.0->sumy) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.7.0->sumy) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.7.0->sumy) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n","Building wheels for collected packages: breadability, docopt, pycountry\n","  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21708 sha256=da09cc6f628309c80eeb83c09f4a5165c9a783a37fe822c0d0b1499372335535\n","  Stored in directory: /root/.cache/pip/wheels/ba/9f/70/7795228568b81b57a8932755938da9fb1f291b0576752604aa\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=031877d3a1644c5c266eec8110c7d8f878c821e4fe7e98d31363af3f087236bc\n","  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n","  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681847 sha256=8ee6e3f882a385a9c861c8220e868367510795b1db312a6fcae1d3bad0209609\n","  Stored in directory: /root/.cache/pip/wheels/47/15/92/e6dc85fcb0686c82e1edbcfdf80cfe4808c058813fed0baa8f\n","Successfully built breadability docopt pycountry\n","Installing collected packages: docopt, pycountry, breadability, sumy\n","Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"]}],"source":["!pip install sumy"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5SXSaE0O1Rk","outputId":"55ba7254-9aeb-49aa-8ce6-8200e564178e","executionInfo":{"status":"ok","timestamp":1681929992068,"user_tz":-330,"elapsed":2770,"user":{"displayName":"Sreya Reddy","userId":"13301942800402694680"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#Code to summarize a given webpage using Sumy's TextRank implementation. \n","from sumy.parsers.html import HtmlParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.text_rank import TextRankSummarizer\n","from sumy.summarizers.lex_rank import LexRankSummarizer\n","from sumy.summarizers.luhn import LuhnSummarizer\n","from sumy.summarizers.lsa import LsaSummarizer\n","\n","num_sentences_in_summary = 2 #getting 2 sentence\n","url = \"https://en.wikipedia.org/wiki/Automatic_summarization\" #URL link\n","parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n","\n","summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n","summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n","\n","for i,summarizer in enumerate(summarizers):\n","    print(summarizer_list[i])\n","    for sentence in summarizer(parser.document, num_sentences_in_summary):\n","        print((sentence))\n","    print(\"-\"*30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uddqGNQOM4V","outputId":"db6375a9-3f33-4369-c0ea-abb6f22f8c7f","executionInfo":{"status":"ok","timestamp":1681930000856,"user_tz":-330,"elapsed":8791,"user":{"displayName":"Sreya Reddy","userId":"13301942800402694680"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TextRankSummarizer:\n","For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n","A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning, To Appear In Proc.\n","------------------------------\n","LexRankSummarizer:\n","The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n","Automatic Text Summarization.\n","------------------------------\n","LuhnSummarizer:\n","Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n","A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning, To Appear In Proc.\n","------------------------------\n","LsaSummarizer\n","Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n","Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n","------------------------------\n"]}]},{"cell_type":"markdown","source":["Summarization with Gensim"],"metadata":{"id":"Sq0cIX31QOWc"}},{"cell_type":"code","source":["!pip install gensim==3.8.3 #installation of the library"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y15IWSTOQI9O","outputId":"32aa1bb4-8bb6-44bb-bcc2-a062423835e1","executionInfo":{"status":"ok","timestamp":1681930064600,"user_tz":-330,"elapsed":63768,"user":{"displayName":"Sreya Reddy","userId":"13301942800402694680"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gensim==3.8.3\n","  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (1.22.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (1.10.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (1.16.0)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (6.3.0)\n","Building wheels for collected packages: gensim\n","  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gensim: filename=gensim-3.8.3-cp39-cp39-linux_x86_64.whl size=26528039 sha256=6c050cf5f07a9ca0294b10761168aaf067022d645e515bdeb9353d83e9f59199\n","  Stored in directory: /root/.cache/pip/wheels/ca/5d/af/618594ec2f28608c1d6ee7d2b7e95a3e9b06551e3b80a491d6\n","Successfully built gensim\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 4.3.1\n","    Uninstalling gensim-4.3.1:\n","      Successfully uninstalled gensim-4.3.1\n","Successfully installed gensim-3.8.3\n"]}]},{"cell_type":"code","source":["from gensim.summarization import summarize,summarize_corpus\n","from gensim.summarization.textcleaner import split_sentences\n","from gensim import corpora\n"],"metadata":{"id":"FFFZzerhQUz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"a5EL7v-x2Kx_"}},{"cell_type":"code","source":["text = open(\"/content/nlp.txt\").read()\n","\n","#summarize method extracts the most relevant sentences in a text\n","print(\"Summarize:\\n\",summarize(text, word_count=200, ratio = 0.1))\n","\n","\n","#the summarize_corpus selects the most important documents in a corpus:\n","sentences = split_sentences(text)# Creates a corpus where each document is a sentence.\n","tokens = [sentence.split() for sentence in sentences]\n","dictionary = corpora.Dictionary(tokens)\n","corpus = [dictionary.doc2bow(sentence_tokens) for sentence_tokens in tokens]\n","\n","# Extracts the most important documents (shown here in BoW representation)\n","print(\"-\"*30,\"\\nSummarize Corpus\\n\",summarize_corpus(corpus,ratio=0.1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZDDdn-rSbuW","outputId":"775244c0-1a25-4cba-e51d-31aa79c89c44","executionInfo":{"status":"ok","timestamp":1681930091635,"user_tz":-330,"elapsed":1027,"user":{"displayName":"Sreya Reddy","userId":"13301942800402694680"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Summarize:\n"," As a computer science student specializing in artificial intelligence and machine learning, I am eager to apply my knowledge and skills to a machine learning development internship role.\n","Relevant coursework: My specialization in artificial intelligence and machine learning has equipped me with the necessary knowledge to understand the fundamental concepts of machine learning.\n","I have taken courses in algorithms, statistics, and probability, which are critical components of machine learning.\n","NLP experience: I have experience in natural language processing, which is a crucial area in machine learning.\n","This experience has helped me develop an understanding of how machine learning algorithms can be used to analyze and process natural language data.\n","I am always eager to learn and stay updated with the latest developments in the field of machine learning.\n","Good communication skills: Effective communication is a critical component of any successful team.\n","As a computer science student, I have honed my communication skills through class presentations, group projects, and discussions.\n","I believe that my communication skills will enable me to work effectively with the team and contribute to the success of the project.\n","Eagerness: I am eager to apply my knowledge and skills to real-world problems and make a meaningful contribution to the field of machine learning.\n","------------------------------ \n","Summarize Corpus\n"," [[(0, 1), (1, 1), (2, 2), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2)]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Hs9fY2vocExS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observations: summary of the given data is displayed as the output\n","Summarization with Sumy Sumy is a simple library and command-line utility for extracting summaries from HTML pages or plain texts. The\n","package also contains a simple evaluation framework for text summaries.\n","Sumy offers several algorithms and methods for summarization such as: Luhn - Heuristic method: Luhn’s algorithm is an approach based\n","on TF-IDF, this is one of the earliest approaches to text summarization.\n","Luhn proposed that the significance of each word in a document signifies how important it is. The idea is that any sentence with\n","maximum occurrences of the highest frequency words and least occurrences are not important to the meaning of the document than\n","others. Although it is not considered a very accurate approach.\n","Latent Semantic Analysis: Latent Semantic Analysis is a technique for creating a vector representation of a document. Having a vector\n","representation of a document gives you a way to compare documents for their similarity by calculating the distance between the vectors.\n","LexRank - Unsupervised approach inspired by algorithm PageRank and HITS.LexRank is an unsupervised graph-based approach for\n","automatic text summarization. The scoring of sentences is done using the graph method. LexRank is used for computing sentence\n","importance based on the concept of eigenvector centrality in a graph representation of sentences.\n","TextRank - TextRank uses an extractive approach and is unsupervised graph-based and PageRank-based for text summarization. In\n","TextRank, the vertices of the graph are sentences, and the edge weights between sentences denote the similarity between sentences.\n","Summarization with Gensim Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. The\n","target audience is the natural language processing (NLP) and information retrieval (IR) community."],"metadata":{"id":"noFxK91DcFz6"}}]}